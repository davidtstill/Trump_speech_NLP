{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Sentiment Analysis and Generating a Word Cloud on Trump Campaign Speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary modules to parse multiple text files into one large dataframe\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in the individual speeches, converting each speech into one large, concatenated string, and lastly storing it in a dataframe\n",
    "\n",
    "text1 = pd.read_csv(Path('speeches/BattleCreekDec19_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df1 = pd.DataFrame([text1], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = pd.read_csv(Path('speeches/BemidjiSep18_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df2 = pd.DataFrame([text2], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = pd.read_csv(Path('speeches/CharlestonFeb28_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df3 = pd.DataFrame([text3], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "text4 = pd.read_csv(Path('speeches/CharlotteMar2_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df4 = pd.DataFrame([text4], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "text5 = pd.read_csv(Path('speeches/CincinnatiAug1_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df5 = pd.DataFrame([text5], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "text6 = pd.read_csv(Path('speeches/ColoradorSpringsFeb20_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df6 = pd.DataFrame([text6], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "text7 = pd.read_csv(Path('speeches/DallasOct17_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df7 = pd.DataFrame([text7], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "text8 = pd.read_csv(Path('speeches/DesMoinesJan30_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df8 = pd.DataFrame([text8], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "text9 = pd.read_csv(Path('speeches/FayettevilleSep9_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df9 = pd.DataFrame([text9], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "text10 = pd.read_csv(Path('speeches/FayettevilleSep19_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df10 = pd.DataFrame([text10], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "text11 = pd.read_csv(Path('speeches/FreelandSep10_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df11 = pd.DataFrame([text11], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "text12 = pd.read_csv(Path('speeches/GreenvilleJul17_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df12 = pd.DataFrame([text12], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "text13 = pd.read_csv(Path('speeches/HendersonSep13_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df13 = pd.DataFrame([text13], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "text14 = pd.read_csv(Path('speeches/HersheyDec10_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df14 = pd.DataFrame([text14], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "text15 = pd.read_csv(Path('speeches/LasVegasFeb21_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df15 = pd.DataFrame([text15], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "text16 = pd.read_csv(Path('speeches/LatrobeSep3_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df16 = pd.DataFrame([text16], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "text17 = pd.read_csv(Path('speeches/LexingtonNov4_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df17 = pd.DataFrame([text17], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "text18 = pd.read_csv(Path('speeches/MilwaukeeJan14_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df18 = pd.DataFrame([text18], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "text19 = pd.read_csv(Path('speeches/MindenSep12_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df19 = pd.DataFrame([text19], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "text20 = pd.read_csv(Path('speeches/MinneapolisOct10_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df20 = pd.DataFrame([text20], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "text21 = pd.read_csv(Path('speeches/MosineeSep17_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df21 = pd.DataFrame([text21], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "text22 = pd.read_csv(Path('speeches/NewHampshireAug15_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df22 = pd.DataFrame([text22], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "text23 = pd.read_csv(Path('speeches/NewHampshireAug28_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df23 = pd.DataFrame([text23], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "text24 = pd.read_csv(Path('speeches/NewHampshireFeb10_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df24 = pd.DataFrame([text24], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "text25 = pd.read_csv(Path('speeches/NewMexicoSep16_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df25 = pd.DataFrame([text25], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "text26 = pd.read_csv(Path('speeches/OhioSep21_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df26 = pd.DataFrame([text26], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "text27 = pd.read_csv(Path('speeches/PhoenixFeb19_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df27 = pd.DataFrame([text27], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "text28 = pd.read_csv(Path('speeches/PittsburghSep22_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df28 = pd.DataFrame([text28], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "text29 = pd.read_csv(Path('speeches/TexasSep23_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df29 = pd.DataFrame([text29], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "text30 = pd.read_csv(Path('speeches/ToledoJan9_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df30 = pd.DataFrame([text30], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "text31 = pd.read_csv(Path('speeches/TulsaJun20_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df31 = pd.DataFrame([text31], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "text32 = pd.read_csv(Path('speeches/TupeloNov1_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df32 = pd.DataFrame([text32], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "text33 = pd.read_csv(Path('speeches/WildwoodJan28_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df33 = pd.DataFrame([text33], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "text34 = pd.read_csv(Path('speeches/Winston-SalemSep8_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df34 = pd.DataFrame([text34], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "text35 = pd.read_csv(Path('speeches/YumaAug18_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df35 = pd.DataFrame([text35], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speech Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you. Thank you. Thank you to Vice President Pence. He's a good guy. We've done a great job together. And Merry Christmas, Michigan. Thank yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There's a lot of people. That's great. Thank you very much. Thank you very much. That's a big group of people. This is on fast notice, too. Thank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thank you. Thank you. Thank you. All I can say is that the fake news just doesn't get it, do they? They don't get it. They just don't get it. Hell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I want to thank you very much. North Carolina, thank you very much. I'm thrilled to back in the great city of Charlotte, where, by the way, we're ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you all. Thank you very much. Thank you to Vice President Mike Pence, and hello Cincinnati. You know, I used to work in Cincinnati, a place ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                             Speech Text\n",
       "0  Thank you. Thank you. Thank you to Vice President Pence. He's a good guy. We've done a great job together. And Merry Christmas, Michigan. Thank yo...\n",
       "1  There's a lot of people. That's great. Thank you very much. Thank you very much. That's a big group of people. This is on fast notice, too. Thank ...\n",
       "2  Thank you. Thank you. Thank you. All I can say is that the fake news just doesn't get it, do they? They don't get it. They just don't get it. Hell...\n",
       "3  I want to thank you very much. North Carolina, thank you very much. I'm thrilled to back in the great city of Charlotte, where, by the way, we're ...\n",
       "4  Thank you all. Thank you very much. Thank you to Vice President Mike Pence, and hello Cincinnati. You know, I used to work in Cincinnati, a place ..."
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, \n",
    "                         df11, df12, df13, df14, df15, df16, df17, df18, df19,\n",
    "                         df20, df21, df22, df23, df24, df25, df6, df27, df28,\n",
    "                         df29, df30, df31, df32, df33, df34, df35], ignore_index=True)\n",
    "pd.set_option('max_colwidth', 150)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speech Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you  Thank you  Thank you to Vice President Pence  He s a good guy  We ve done a great job together  And Merry Christmas  Michigan  Thank yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There s a lot of people  That s great  Thank you very much  Thank you very much  That s a big group of people  This is on fast notice  too  Thank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thank you  Thank you  Thank you  All I can say is that the fake news just doesn t get it  do they  They don t get it  They just don t get it  Hell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I want to thank you very much  North Carolina  thank you very much  I m thrilled to back in the great city of Charlotte  where  by the way  we re ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you all  Thank you very much  Thank you to Vice President Mike Pence  and hello Cincinnati  You know  I used to work in Cincinnati  a place ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                             Speech Text\n",
       "0  Thank you  Thank you  Thank you to Vice President Pence  He s a good guy  We ve done a great job together  And Merry Christmas  Michigan  Thank yo...\n",
       "1  There s a lot of people  That s great  Thank you very much  Thank you very much  That s a big group of people  This is on fast notice  too  Thank ...\n",
       "2  Thank you  Thank you  Thank you  All I can say is that the fake news just doesn t get it  do they  They don t get it  They just don t get it  Hell...\n",
       "3  I want to thank you very much  North Carolina  thank you very much  I m thrilled to back in the great city of Charlotte  where  by the way  we re ...\n",
       "4  Thank you all  Thank you very much  Thank you to Vice President Mike Pence  and hello Cincinnati  You know  I used to work in Cincinnati  a place ..."
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Begin to clean the the text by removing punctuations\n",
    "\n",
    "combined_df.replace(\"[^a-zA-Z]\",\" \",regex=True, inplace=True)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing the Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary modules\n",
    "\n",
    "from nltk.corpus import stopwords, reuters\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the VADER sentiment analyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment calculation based on compound score\n",
    "\n",
    "def get_sentiment(score):\n",
    "    \"\"\"\n",
    "    Calculates the sentiment based on the compound score.\n",
    "    \"\"\"\n",
    "    result = 0  # Neutral by default\n",
    "    if score >= 0.05:  # Positive\n",
    "        result = 1\n",
    "    elif score <= -0.05:  # Negative\n",
    "        result = -1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment scores dictionaries\n",
    "\n",
    "speech_text = {\n",
    "    \"compound\": [],\n",
    "    \"positive\": [],\n",
    "    \"neutral\": [],\n",
    "    \"negative\": [],\n",
    "    \"sent\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sentiment for the text and the title\n",
    "\n",
    "for index, row in combined_df.iterrows():\n",
    "    try:\n",
    "        # Sentiment scoring with VADER\n",
    "        speech_sentiment = analyzer.polarity_scores(row[\"Speech Text\"])\n",
    "        speech_text[\"compound\"].append(speech_sentiment[\"compound\"])\n",
    "        speech_text[\"positive\"].append(speech_sentiment[\"pos\"])\n",
    "        speech_text[\"neutral\"].append(speech_sentiment[\"neu\"])\n",
    "        speech_text[\"negative\"].append(speech_sentiment[\"neg\"])\n",
    "        speech_text[\"sent\"].append(get_sentiment(speech_sentiment[\"compound\"]))\n",
    "    except AttributeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compound</th>\n",
       "      <th>positive</th>\n",
       "      <th>neutral</th>\n",
       "      <th>negative</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.093</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.093</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.093</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.105</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.099</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   compound  positive  neutral  negative  sent\n",
       "0       1.0     0.169    0.738     0.093   1.0\n",
       "1       1.0     0.169    0.738     0.093   1.0\n",
       "2       1.0     0.174    0.733     0.093   1.0\n",
       "3       1.0     0.193    0.701     0.105   1.0\n",
       "4       1.0     0.189    0.711     0.099   1.0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attaching sentiment scores to a new DataFrame\n",
    "\n",
    "df_scores = pd.DataFrame.from_dict(speech_text, orient='index')\n",
    "df_scores = df_scores.transpose()\n",
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speech Text</th>\n",
       "      <th>compound</th>\n",
       "      <th>positive</th>\n",
       "      <th>neutral</th>\n",
       "      <th>negative</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you  Thank you  Thank you to Vice President Pence  He s a good guy  We ve done a great job together  And Merry Christmas  Michigan  Thank yo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.093</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There s a lot of people  That s great  Thank you very much  Thank you very much  That s a big group of people  This is on fast notice  too  Thank ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.093</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thank you  Thank you  Thank you  All I can say is that the fake news just doesn t get it  do they  They don t get it  They just don t get it  Hell...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.093</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I want to thank you very much  North Carolina  thank you very much  I m thrilled to back in the great city of Charlotte  where  by the way  we re ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.105</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you all  Thank you very much  Thank you to Vice President Mike Pence  and hello Cincinnati  You know  I used to work in Cincinnati  a place ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.099</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                             Speech Text  \\\n",
       "0  Thank you  Thank you  Thank you to Vice President Pence  He s a good guy  We ve done a great job together  And Merry Christmas  Michigan  Thank yo...   \n",
       "1  There s a lot of people  That s great  Thank you very much  Thank you very much  That s a big group of people  This is on fast notice  too  Thank ...   \n",
       "2  Thank you  Thank you  Thank you  All I can say is that the fake news just doesn t get it  do they  They don t get it  They just don t get it  Hell...   \n",
       "3  I want to thank you very much  North Carolina  thank you very much  I m thrilled to back in the great city of Charlotte  where  by the way  we re ...   \n",
       "4  Thank you all  Thank you very much  Thank you to Vice President Mike Pence  and hello Cincinnati  You know  I used to work in Cincinnati  a place ...   \n",
       "\n",
       "   compound  positive  neutral  negative  sent  \n",
       "0       1.0     0.169    0.738     0.093   1.0  \n",
       "1       1.0     0.169    0.738     0.093   1.0  \n",
       "2       1.0     0.174    0.733     0.093   1.0  \n",
       "3       1.0     0.193    0.701     0.105   1.0  \n",
       "4       1.0     0.189    0.711     0.099   1.0  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenating the df_scores DataFrame with the original DataFrame that contained only text\n",
    "\n",
    "new_df = combined_df.join(df_scores)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compound</th>\n",
       "      <th>positive</th>\n",
       "      <th>neutral</th>\n",
       "      <th>negative</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.00000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.18560</td>\n",
       "      <td>0.717400</td>\n",
       "      <td>0.096886</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.01782</td>\n",
       "      <td>0.015545</td>\n",
       "      <td>0.014527</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.15600</td>\n",
       "      <td>0.672000</td>\n",
       "      <td>0.042000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.17200</td>\n",
       "      <td>0.710500</td>\n",
       "      <td>0.092500</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.18900</td>\n",
       "      <td>0.718000</td>\n",
       "      <td>0.097000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.19300</td>\n",
       "      <td>0.724500</td>\n",
       "      <td>0.103500</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.24100</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.122000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        compound  positive    neutral   negative  sent\n",
       "count  35.000000  35.00000  35.000000  35.000000  35.0\n",
       "mean    0.999994   0.18560   0.717400   0.096886   1.0\n",
       "std     0.000024   0.01782   0.015545   0.014527   0.0\n",
       "min     0.999900   0.15600   0.672000   0.042000   1.0\n",
       "25%     1.000000   0.17200   0.710500   0.092500   1.0\n",
       "50%     1.000000   0.18900   0.718000   0.097000   1.0\n",
       "75%     1.000000   0.19300   0.724500   0.103500   1.0\n",
       "max     1.000000   0.24100   0.750000   0.122000   1.0"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Trump's campaign speeches with a world cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary modules\n",
    "\n",
    "from nltk.corpus import stopwords, reuters\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(doc):\n",
    "    sw = set(stopwords.words('english'))\n",
    "    regex = re.compile(\"[^a-zA-Z ]\")\n",
    "    re_clean = regex.sub('', doc)\n",
    "    words = word_tokenize(re_clean)\n",
    "    lem = [lemmatizer.lemmatize(word) for word in words]\n",
    "    output = [word.lower() for word in lem if word.lower() not in sw]\n",
    "    return ' '.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'speeches_string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-2c048c01effc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Process text for wordcloud creation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbig_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspeeches_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0minput_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbig_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'speeches_string' is not defined"
     ]
    }
   ],
   "source": [
    "# Process text for wordcloud creation\n",
    "big_string = ' '.join(speeches_string)\n",
    "input_text = process_text(big_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(width=3000, height=1500, max_words=30).generate()\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
