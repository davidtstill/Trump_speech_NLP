{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Sentiment Analysis and Generating a Word Cloud on Trump Campaign Speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary modules to parse multiple text files into one large dataframe\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in the individual speeches, converting each speech into one large, concatenated string, and lastly storing it in a dataframe\n",
    "\n",
    "text1 = pd.read_csv(Path('speeches/BattleCreekDec19_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df1 = pd.DataFrame([text1], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = pd.read_csv(Path('speeches/BemidjiSep18_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df2 = pd.DataFrame([text2], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = pd.read_csv(Path('speeches/CharlestonFeb28_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df3 = pd.DataFrame([text3], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "text4 = pd.read_csv(Path('speeches/CharlotteMar2_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df4 = pd.DataFrame([text4], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "text5 = pd.read_csv(Path('speeches/CincinnatiAug1_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df5 = pd.DataFrame([text5], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "text6 = pd.read_csv(Path('speeches/ColoradorSpringsFeb20_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df6 = pd.DataFrame([text6], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "text7 = pd.read_csv(Path('speeches/DallasOct17_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df7 = pd.DataFrame([text7], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "text8 = pd.read_csv(Path('speeches/DesMoinesJan30_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df8 = pd.DataFrame([text8], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "text9 = pd.read_csv(Path('speeches/FayettevilleSep9_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df9 = pd.DataFrame([text9], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "text10 = pd.read_csv(Path('speeches/FayettevilleSep19_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df10 = pd.DataFrame([text10], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "text11 = pd.read_csv(Path('speeches/FreelandSep10_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df11 = pd.DataFrame([text11], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "text12 = pd.read_csv(Path('speeches/GreenvilleJul17_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df12 = pd.DataFrame([text12], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "text13 = pd.read_csv(Path('speeches/HendersonSep13_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df13 = pd.DataFrame([text13], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "text14 = pd.read_csv(Path('speeches/HersheyDec10_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df14 = pd.DataFrame([text14], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "text15 = pd.read_csv(Path('speeches/LasVegasFeb21_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df15 = pd.DataFrame([text15], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "text16 = pd.read_csv(Path('speeches/LatrobeSep3_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df16 = pd.DataFrame([text16], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "text17 = pd.read_csv(Path('speeches/LexingtonNov4_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df17 = pd.DataFrame([text17], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "text18 = pd.read_csv(Path('speeches/MilwaukeeJan14_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df18 = pd.DataFrame([text18], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "text19 = pd.read_csv(Path('speeches/MindenSep12_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df19 = pd.DataFrame([text19], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "text20 = pd.read_csv(Path('speeches/MinneapolisOct10_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df20 = pd.DataFrame([text20], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "text21 = pd.read_csv(Path('speeches/MosineeSep17_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df21 = pd.DataFrame([text21], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "text22 = pd.read_csv(Path('speeches/NewHampshireAug15_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df22 = pd.DataFrame([text22], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "text23 = pd.read_csv(Path('speeches/NewHampshireAug28_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df23 = pd.DataFrame([text23], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "text24 = pd.read_csv(Path('speeches/NewHampshireFeb10_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df24 = pd.DataFrame([text24], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "text25 = pd.read_csv(Path('speeches/NewMexicoSep16_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df25 = pd.DataFrame([text25], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "text26 = pd.read_csv(Path('speeches/OhioSep21_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df26 = pd.DataFrame([text26], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "text27 = pd.read_csv(Path('speeches/PhoenixFeb19_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df27 = pd.DataFrame([text27], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "text28 = pd.read_csv(Path('speeches/PittsburghSep22_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df28 = pd.DataFrame([text28], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "text29 = pd.read_csv(Path('speeches/TexasSep23_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df29 = pd.DataFrame([text29], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "text30 = pd.read_csv(Path('speeches/ToledoJan9_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df30 = pd.DataFrame([text30], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "text31 = pd.read_csv(Path('speeches/TulsaJun20_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df31 = pd.DataFrame([text31], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "text32 = pd.read_csv(Path('speeches/TupeloNov1_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df32 = pd.DataFrame([text32], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "text33 = pd.read_csv(Path('speeches/WildwoodJan28_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df33 = pd.DataFrame([text33], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "text34 = pd.read_csv(Path('speeches/Winston-SalemSep8_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df34 = pd.DataFrame([text34], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "text35 = pd.read_csv(Path('speeches/YumaAug18_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df35 = pd.DataFrame([text35], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speech Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you. Thank you. Thank you to Vice President Pence. He's a good guy. We've done a great job together. And Merry Christmas, Michigan. Thank yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There's a lot of people. That's great. Thank you very much. Thank you very much. That's a big group of people. This is on fast notice, too. Thank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thank you. Thank you. Thank you. All I can say is that the fake news just doesn't get it, do they? They don't get it. They just don't get it. Hell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I want to thank you very much. North Carolina, thank you very much. I'm thrilled to back in the great city of Charlotte, where, by the way, we're ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you all. Thank you very much. Thank you to Vice President Mike Pence, and hello Cincinnati. You know, I used to work in Cincinnati, a place ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                             Speech Text\n",
       "0  Thank you. Thank you. Thank you to Vice President Pence. He's a good guy. We've done a great job together. And Merry Christmas, Michigan. Thank yo...\n",
       "1  There's a lot of people. That's great. Thank you very much. Thank you very much. That's a big group of people. This is on fast notice, too. Thank ...\n",
       "2  Thank you. Thank you. Thank you. All I can say is that the fake news just doesn't get it, do they? They don't get it. They just don't get it. Hell...\n",
       "3  I want to thank you very much. North Carolina, thank you very much. I'm thrilled to back in the great city of Charlotte, where, by the way, we're ...\n",
       "4  Thank you all. Thank you very much. Thank you to Vice President Mike Pence, and hello Cincinnati. You know, I used to work in Cincinnati, a place ..."
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, \n",
    "                         df11, df12, df13, df14, df15, df16, df17, df18, df19,\n",
    "                         df20, df21, df22, df23, df24, df25, df6, df27, df28,\n",
    "                         df29, df30, df31, df32, df33, df34, df35], ignore_index=True)\n",
    "pd.set_option('max_colwidth', 150)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speech Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you  Thank you  Thank you to Vice Presid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There s a lot of people  That s great  Thank y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thank you  Thank you  Thank you  All I can say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I want to thank you very much  North Carolina ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you all  Thank you very much  Thank you ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Speech Text\n",
       "0  Thank you  Thank you  Thank you to Vice Presid...\n",
       "1  There s a lot of people  That s great  Thank y...\n",
       "2  Thank you  Thank you  Thank you  All I can say...\n",
       "3  I want to thank you very much  North Carolina ...\n",
       "4  Thank you all  Thank you very much  Thank you ..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Begin to clean the the text by removing punctuations\n",
    "\n",
    "combined_df.replace(\"[^a-zA-Z]\",\" \",regex=True, inplace=True)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing the Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary modules\n",
    "\n",
    "from nltk.corpus import stopwords, reuters\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment calculation based on compound score\n",
    "\n",
    "def get_sentiment(score):\n",
    "    \"\"\"\n",
    "    Calculates the sentiment based on the compound score.\n",
    "    \"\"\"\n",
    "    result = 0  # Neutral by default\n",
    "    if score >= 0.05:  # Positive\n",
    "        result = 1\n",
    "    elif score <= -0.05:  # Negative\n",
    "        result = -1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment scores dictionaries\n",
    "\n",
    "speech_text = {\n",
    "    \"compound\": [],\n",
    "    \"positive\": [],\n",
    "    \"neutral\": [],\n",
    "    \"negative\": [],\n",
    "    \"sent\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sentiment for the text and the title\n",
    "\n",
    "for index, row in combined_df.iterrows():\n",
    "    try:\n",
    "        # Sentiment scoring with VADER\n",
    "        speech_sentiment = analyzer.polarity_scores(row[\"title\"])\n",
    "        speech_text[\"title_compound\"].append(title_sentiment[\"compound\"])\n",
    "        speech_text[\"title_pos\"].append(title_sentiment[\"pos\"])\n",
    "        speech_text[\"title_neu\"].append(title_sentiment[\"neu\"])\n",
    "        speech_text[\"title_neg\"].append(title_sentiment[\"neg\"])\n",
    "        speech_text[\"title_sent\"].append(get_sentiment(title_sentiment[\"compound\"]))\n",
    "    except AttributeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_sentiment_df = pd.DataFrame(title_sent)\n",
    "combined_df = combined_df.join(title_sentiment_df).join(text_sentiment_df)\n",
    "\n",
    "news_en_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Trump's campaign speeches with a world cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary modules\n",
    "\n",
    "from nltk.corpus import stopwords, reuters\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(doc):\n",
    "    sw = set(stopwords.words('english'))\n",
    "    regex = re.compile(\"[^a-zA-Z ]\")\n",
    "    re_clean = regex.sub('', doc)\n",
    "    words = word_tokenize(re_clean)\n",
    "    lem = [lemmatizer.lemmatize(word) for word in words]\n",
    "    output = [word.lower() for word in lem if word.lower() not in sw]\n",
    "    return ' '.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process text for wordcloud creation\n",
    "big_string = ' '.join(speeches_string)\n",
    "input_text = process_text(big_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-3966d6c5f239>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombined_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m         \"\"\"\n\u001b[1;32m--> 631\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    610\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m         \"\"\"\n\u001b[1;32m--> 612\u001b[1;33m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    613\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mprocess_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[0mregexp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregexp\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregexp\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34mr\"\\w[\\w']+\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 574\u001b[1;33m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregexp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575\u001b[0m         \u001b[1;31m# remove 's\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m         words = [word[:-2] if word.lower().endswith(\"'s\") else word\n",
      "\u001b[1;32m~\\anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36mfindall\u001b[1;34m(pattern, string, flags)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     Empty matches are included in the result.\"\"\"\n\u001b[1;32m--> 223\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "wc = WordCloud(width=3000, height=1500, max_words=30).generate()\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
