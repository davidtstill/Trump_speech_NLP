{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Sentiment Analysis and Generating a Word Cloud on Trump Campaign Speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary modules to parse multiple text files into one large dataframe\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in the individual speeches, converting each speech into one large, concatenated string, and lastly storing it in a dataframe\n",
    "\n",
    "text1 = pd.read_csv(Path('speeches/BattleCreekDec19_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df1 = pd.DataFrame([text1], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = pd.read_csv(Path('speeches/BemidjiSep18_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df2 = pd.DataFrame([text2], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = pd.read_csv(Path('speeches/CharlestonFeb28_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df3 = pd.DataFrame([text3], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text4 = pd.read_csv(Path('speeches/CharlotteMar2_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df4 = pd.DataFrame([text4], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text5 = pd.read_csv(Path('speeches/CincinnatiAug1_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df5 = pd.DataFrame([text5], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text6 = pd.read_csv(Path('speeches/ColoradorSpringsFeb20_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df6 = pd.DataFrame([text6], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text7 = pd.read_csv(Path('speeches/DallasOct17_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df7 = pd.DataFrame([text7], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text8 = pd.read_csv(Path('speeches/DesMoinesJan30_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df8 = pd.DataFrame([text8], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text9 = pd.read_csv(Path('speeches/FayettevilleSep9_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df9 = pd.DataFrame([text9], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text10 = pd.read_csv(Path('speeches/FayettevilleSep19_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df10 = pd.DataFrame([text10], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text11 = pd.read_csv(Path('speeches/FreelandSep10_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df11 = pd.DataFrame([text11], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text12 = pd.read_csv(Path('speeches/GreenvilleJul17_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df12 = pd.DataFrame([text12], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text13 = pd.read_csv(Path('speeches/HendersonSep13_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df13 = pd.DataFrame([text13], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text14 = pd.read_csv(Path('speeches/HersheyDec10_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df14 = pd.DataFrame([text14], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text15 = pd.read_csv(Path('speeches/LasVegasFeb21_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df15 = pd.DataFrame([text15], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text16 = pd.read_csv(Path('speeches/LatrobeSep3_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df16 = pd.DataFrame([text16], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text17 = pd.read_csv(Path('speeches/LexingtonNov4_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df17 = pd.DataFrame([text17], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text18 = pd.read_csv(Path('speeches/MilwaukeeJan14_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df18 = pd.DataFrame([text18], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text19 = pd.read_csv(Path('speeches/MindenSep12_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df19 = pd.DataFrame([text19], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text20 = pd.read_csv(Path('speeches/MinneapolisOct10_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df20 = pd.DataFrame([text20], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text21 = pd.read_csv(Path('speeches/MosineeSep17_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df21 = pd.DataFrame([text21], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text22 = pd.read_csv(Path('speeches/NewHampshireAug15_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df22 = pd.DataFrame([text22], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text23 = pd.read_csv(Path('speeches/NewHampshireAug28_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df23 = pd.DataFrame([text23], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text24 = pd.read_csv(Path('speeches/NewHampshireFeb10_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df24 = pd.DataFrame([text24], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text25 = pd.read_csv(Path('speeches/NewMexicoSep16_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df25 = pd.DataFrame([text25], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text26 = pd.read_csv(Path('speeches/OhioSep21_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df26 = pd.DataFrame([text26], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text27 = pd.read_csv(Path('speeches/PhoenixFeb19_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df27 = pd.DataFrame([text27], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text28 = pd.read_csv(Path('speeches/PittsburghSep22_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df28 = pd.DataFrame([text28], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text29 = pd.read_csv(Path('speeches/TexasSep23_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df29 = pd.DataFrame([text29], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text30 = pd.read_csv(Path('speeches/ToledoJan9_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df30 = pd.DataFrame([text30], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text31 = pd.read_csv(Path('speeches/TulsaJun20_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df31 = pd.DataFrame([text31], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text32 = pd.read_csv(Path('speeches/TupeloNov1_2019.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df32 = pd.DataFrame([text32], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text33 = pd.read_csv(Path('speeches/WildwoodJan28_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df33 = pd.DataFrame([text33], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text34 = pd.read_csv(Path('speeches/Winston-SalemSep8_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df34 = pd.DataFrame([text34], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text35 = pd.read_csv(Path('speeches/YumaAug18_2020.txt'), sep='\\n', header=None)[0].str.cat()\n",
    "df35 = pd.DataFrame([text35], columns=['Speech Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speech Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you. Thank you. Thank you to Vice President Pence. He's a good guy. We've done a great job together. And Merry Christmas, Michigan. Thank yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There's a lot of people. That's great. Thank you very much. Thank you very much. That's a big group of people. This is on fast notice, too. Thank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thank you. Thank you. Thank you. All I can say is that the fake news just doesn't get it, do they? They don't get it. They just don't get it. Hell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I want to thank you very much. North Carolina, thank you very much. I'm thrilled to back in the great city of Charlotte, where, by the way, we're ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you all. Thank you very much. Thank you to Vice President Mike Pence, and hello Cincinnati. You know, I used to work in Cincinnati, a place ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                             Speech Text\n",
       "0  Thank you. Thank you. Thank you to Vice President Pence. He's a good guy. We've done a great job together. And Merry Christmas, Michigan. Thank yo...\n",
       "1  There's a lot of people. That's great. Thank you very much. Thank you very much. That's a big group of people. This is on fast notice, too. Thank ...\n",
       "2  Thank you. Thank you. Thank you. All I can say is that the fake news just doesn't get it, do they? They don't get it. They just don't get it. Hell...\n",
       "3  I want to thank you very much. North Carolina, thank you very much. I'm thrilled to back in the great city of Charlotte, where, by the way, we're ...\n",
       "4  Thank you all. Thank you very much. Thank you to Vice President Mike Pence, and hello Cincinnati. You know, I used to work in Cincinnati, a place ..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, \n",
    "                         df11, df12, df13, df14, df15, df16, df17, df18, df19,\n",
    "                         df20, df21, df22, df23, df24, df25, df6, df27, df28,\n",
    "                         df29, df30, df31, df32, df33, df34, df35], ignore_index=True)\n",
    "pd.set_option('max_colwidth', 150)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speech Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thank you  Thank you  Thank you to Vice President Pence  He s a good guy  We ve done a great job together  And Merry Christmas  Michigan  Thank yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There s a lot of people  That s great  Thank you very much  Thank you very much  That s a big group of people  This is on fast notice  too  Thank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thank you  Thank you  Thank you  All I can say is that the fake news just doesn t get it  do they  They don t get it  They just don t get it  Hell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I want to thank you very much  North Carolina  thank you very much  I m thrilled to back in the great city of Charlotte  where  by the way  we re ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you all  Thank you very much  Thank you to Vice President Mike Pence  and hello Cincinnati  You know  I used to work in Cincinnati  a place ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                             Speech Text\n",
       "0  Thank you  Thank you  Thank you to Vice President Pence  He s a good guy  We ve done a great job together  And Merry Christmas  Michigan  Thank yo...\n",
       "1  There s a lot of people  That s great  Thank you very much  Thank you very much  That s a big group of people  This is on fast notice  too  Thank ...\n",
       "2  Thank you  Thank you  Thank you  All I can say is that the fake news just doesn t get it  do they  They don t get it  They just don t get it  Hell...\n",
       "3  I want to thank you very much  North Carolina  thank you very much  I m thrilled to back in the great city of Charlotte  where  by the way  we re ...\n",
       "4  Thank you all  Thank you very much  Thank you to Vice President Mike Pence  and hello Cincinnati  You know  I used to work in Cincinnati  a place ..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Begin to clean the the text by removing punctuations\n",
    "\n",
    "combined_df.replace(\"[^a-zA-Z]\",\" \",regex=True, inplace=True)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing the Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary modules\n",
    "\n",
    "from nltk.corpus import stopwords, reuters\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the VADER sentiment analyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment calculation based on compound score\n",
    "\n",
    "def get_sentiment(score):\n",
    "    \"\"\"\n",
    "    Calculates the sentiment based on the compound score.\n",
    "    \"\"\"\n",
    "    result = 0  # Neutral by default\n",
    "    if score >= 0.05:  # Positive\n",
    "        result = 1\n",
    "    elif score <= -0.05:  # Negative\n",
    "        result = -1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment scores dictionaries\n",
    "\n",
    "speech_text = {\n",
    "    \"compound\": [],\n",
    "    \"positive\": [],\n",
    "    \"neutral\": [],\n",
    "    \"negative\": [],\n",
    "    \"sent\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sentiment for the text and the title\n",
    "\n",
    "for index, row in combined_df.iterrows():\n",
    "    try:\n",
    "        # Sentiment scoring with VADER\n",
    "        speech_sentiment = analyzer.polarity_scores(row[\"Speech Text\"])\n",
    "        speech_text[\"compound\"].append(speech_sentiment[\"compound\"])\n",
    "        speech_text[\"positive\"].append(speech_sentiment[\"pos\"])\n",
    "        speech_text[\"neutral\"].append(speech_sentiment[\"neu\"])\n",
    "        speech_text[\"negative\"].append(speech_sentiment[\"neg\"])\n",
    "    except AttributeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compound</th>\n",
       "      <th>positive</th>\n",
       "      <th>neutral</th>\n",
       "      <th>negative</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.097</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.095</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.092</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.099</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.109</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    compound  positive  neutral  negative  sent\n",
       "30    1.0000     0.172    0.731     0.097   1.0\n",
       "31    1.0000     0.192    0.712     0.095   1.0\n",
       "32    1.0000     0.206    0.702     0.092   1.0\n",
       "33    1.0000     0.165    0.736     0.099   1.0\n",
       "34    0.9999     0.156    0.734     0.109   1.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame.from_dict(speech_text, orient='index')\n",
    "df_test = df_test.transpose()\n",
    "df_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "columns overlap but no suffix specified: Index(['compound', 'positive', 'neutral', 'negative', 'sent'], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-343d1b32620c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnew_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[0;32m   7207\u001b[0m         \"\"\"\n\u001b[0;32m   7208\u001b[0m         return self._join_compat(\n\u001b[1;32m-> 7209\u001b[1;33m             \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlsuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlsuffix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrsuffix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7210\u001b[0m         )\n\u001b[0;32m   7211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_join_compat\u001b[1;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[0;32m   7230\u001b[0m                 \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7231\u001b[0m                 \u001b[0msuffixes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlsuffix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7232\u001b[1;33m                 \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7233\u001b[0m             )\n\u001b[0;32m   7234\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     )\n\u001b[1;32m---> 88\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         llabels, rlabels = _items_overlap_with_suffix(\n\u001b[1;32m--> 649\u001b[1;33m             \u001b[0mldata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlsuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrsuf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m         )\n\u001b[0;32m    651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_items_overlap_with_suffix\u001b[1;34m(left, lsuffix, right, rsuffix)\u001b[0m\n\u001b[0;32m   2024\u001b[0m         raise ValueError(\n\u001b[0;32m   2025\u001b[0m             \u001b[1;34m\"columns overlap but no suffix specified: \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2026\u001b[1;33m             \u001b[1;34m\"{rename}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mto_rename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2027\u001b[0m         )\n\u001b[0;32m   2028\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: columns overlap but no suffix specified: Index(['compound', 'positive', 'neutral', 'negative', 'sent'], dtype='object')"
     ]
    }
   ],
   "source": [
    "new_df = combined_df.join(df_test)\n",
    "\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Trump's campaign speeches with a world cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary modules\n",
    "\n",
    "from nltk.corpus import stopwords, reuters\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(doc):\n",
    "    sw = set(stopwords.words('english'))\n",
    "    regex = re.compile(\"[^a-zA-Z ]\")\n",
    "    re_clean = regex.sub('', doc)\n",
    "    words = word_tokenize(re_clean)\n",
    "    lem = [lemmatizer.lemmatize(word) for word in words]\n",
    "    output = [word.lower() for word in lem if word.lower() not in sw]\n",
    "    return ' '.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'speeches_string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-2c048c01effc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Process text for wordcloud creation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbig_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspeeches_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0minput_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbig_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'speeches_string' is not defined"
     ]
    }
   ],
   "source": [
    "# Process text for wordcloud creation\n",
    "big_string = ' '.join(speeches_string)\n",
    "input_text = process_text(big_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(width=3000, height=1500, max_words=30).generate()\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
